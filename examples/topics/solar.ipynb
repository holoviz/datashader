{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Solar Radiation Data\n",
    "\n",
    "The data in this notebook come from the [National Solar Radiation Data Base](http://rredc.nrel.gov/solar/old_data/nsrdb/), specifically the [1991 - 2010 update to the National Solar Radiation Database](http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2010/).  The data set consists of CSV files [measured at USAF weather stations](http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2010/hourly/list_by_USAFN.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the `download_sample_data.py` script to download Lidar from [Puget Sound LiDAR consortium](http://pugetsoundlidar.ess.washington.edu) and other example data sets.  \n",
    "\n",
    "From your local clone of the `datashader` repository:\n",
    "```\n",
    "cd examples\n",
    "conda env create environment.yml\n",
    "source activate ds \n",
    "python download_sample_data.py\n",
    "```\n",
    "Note on Windows, replace `source activate ds` with `activate ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from dask.distributed import Client\n",
    "from holoviews.operation import decimate\n",
    "from holoviews.operation.datashader import dynspread\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import holoviews as hv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "hv.notebook_extension('bokeh')\n",
    "decimate.max_samples=1000\n",
    "dynspread.max_px=20\n",
    "dynspread.threshold=0.5\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STATIONS = None # adjust to and integer limit to subset of SOLAR_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLAR_FNAME_PATTERN = os.path.join('..', 'data', '72*', '*solar.csv')\n",
    "SOLAR_FILES = glob.glob(SOLAR_FNAME_PATTERN)\n",
    "META_FILE = os.path.join('..', 'data', 'NSRDB_StationsMeta.csv')\n",
    "\n",
    "get_station_yr = lambda fname: tuple(map(int, os.path.basename(fname).split('_')[:2]))\n",
    "STATION_COMBOS = defaultdict(lambda: [])\n",
    "for fname in SOLAR_FILES:\n",
    "    k, v = get_station_yr(fname)\n",
    "    STATION_COMBOS[k].append([v, fname])\n",
    "choices = tuple(STATION_COMBOS)\n",
    "if NUM_STATIONS:\n",
    "    choices = choices[:NUM_STATIONS]\n",
    "STATION_COMBOS = {k: STATION_COMBOS[k] for k in choices}\n",
    "files_for_station = lambda station: [x[1] for x in STATION_COMBOS[station]]\n",
    "station_year_files = lambda station, year: [x for x in files_for_station(station) if '_{}_'.format(year) in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col_names(dframe):\n",
    "    cols = [re.sub('_$', '', re.sub('[/:\\(\\)_\\s^-]+', '_', col.replace('%', '_pcent_'))).lower()\n",
    "            for col in dframe.columns]\n",
    "    dframe.columns = cols\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = clean_col_names(pd.read_csv(META_FILE, index_col='USAF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.loc[list(STATION_COMBOS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['date', 'y', 'x', 'julian_hr', 'year', 'usaf', 'month', 'hour']\n",
    "\n",
    "@dask.delayed\n",
    "def read_one_fname(usaf_station, fname):\n",
    "    dframe = clean_col_names(pd.read_csv(fname))\n",
    "    station_data = meta_df.loc[usaf_station]\n",
    "    hour_offset = dframe.hh_mm_lst.map(lambda x: pd.Timedelta(hours=int(x.split(':')[0])))   \n",
    "    keep = keep_cols + [col for col in dframe.columns\n",
    "                        if ('metstat' in col or col in keep_cols)\n",
    "                        and 'flg' not in col]\n",
    "    dframe['date'] = pd.to_datetime(dframe.yyyy_mm_dd) + hour_offset\n",
    "    dframe['month'] = dframe.date.dt.month\n",
    "    dframe['hour'] = dframe.date.dt.hour\n",
    "    dframe['usaf'] = usaf_station\n",
    "    dframe['y'], dframe['x'] = station_data.nsrdb_lat_dd, station_data.nsrdb_lon_dd \n",
    "    dframe['julian_hr'] = dframe.date.dt.hour + (dframe.date.dt.dayofyear - 1) * 24\n",
    "    dframe['year'] = dframe.date.dt.year\n",
    "    dframe[dframe <= -999] = np.NaN\n",
    "    return dframe.loc[:, keep]\n",
    "\n",
    "def read_one_station(station):\n",
    "    '''Read one USAF station's 1991 to 2001 CSVs - dask.delayed for each each year'''\n",
    "    files = files_for_station(station)\n",
    "    return dd.from_delayed([read_one_fname(station, fname) for fname in files]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_usaf = tuple(STATION_COMBOS)[0]\n",
    "df = read_one_station(example_usaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df.date.describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell makes some labels for the time series groupby operations' plots and boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct, dif_h, glo_h = ('Direct Normal', \n",
    "                        'Diffuse Horizontal', \n",
    "                        'Global Horizontal',)\n",
    "labels = {}\n",
    "watt_hrs_m2_cols = [col for col in df.columns if 'wh_m_2' in col and not 'suny' in col]\n",
    "for col in watt_hrs_m2_cols:\n",
    "    label_1 = \"Clear Sky \" if 'csky' in col else \"Measured \"\n",
    "    label_2 = direct if '_dir_' in col else glo_h if '_glo_' in col else dif_h\n",
    "    labels[col] = label_1 + label_2\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_quantiles(station=None, grouper='julian_hr', usaf_data=None):\n",
    "    '''Given a station name or dataframe do groupby on time bins\n",
    "    Parameters:\n",
    "        station:    Integer name of a USAF weather station \n",
    "                    (folder names holding years' CSVs)\n",
    "        groupby:    One of \"julian_hr\" \"hour\" \"month_hour\"\n",
    "                    (Note the julian_hr does not standardize relative to leap\n",
    "                    years: non-leap years have 8760 hrs, leap years 8784 hrs)\n",
    "        usaf_data:  Give CSVs' dataframe instead of station name\n",
    "    Returns:\n",
    "        summary_df  Dataframe with 25%, 50%, 75% for each column\n",
    "    '''\n",
    "\n",
    "    if usaf_data is None:\n",
    "        usaf_data = read_one_station(station)\n",
    "    if grouper == 'hour':\n",
    "        group_var = usaf_data.date.dt.hour\n",
    "    elif grouper == 'month':\n",
    "        group_var = usaf_data.date.dt.month\n",
    "    elif grouper == 'month_hour':\n",
    "        group_var = [usaf_data.date.dt.month, usaf_data.date.dt.hour]\n",
    "    else:\n",
    "        group_var = grouper\n",
    "    usaf_data = usaf_data.groupby(group_var)\n",
    "    usaf_data = usaf_data[keep_cols + watt_hrs_m2_cols]\n",
    "    low = usaf_data.quantile(0.25)\n",
    "    median = usaf_data.median()\n",
    "    hi = usaf_data.quantile(0.75)\n",
    "    median[grouper] = median.index.values\n",
    "    median['usaf'] = station\n",
    "    # For the low, hi quartiles subset the columns\n",
    "    # for smaller joins - do not include 3 copies of x,y,date, etc\n",
    "    join_arg_cols = [col for col in low.columns if col not in keep_cols]\n",
    "    summary_df = median.join(low[join_arg_cols], \n",
    "                             rsuffix='_low').join(hi[join_arg_cols], rsuffix='_hi')\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Julian day of year summary for one USAF station using `pandas.DataFrame.groupby`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julian_summary = get_station_quantiles(station=example_usaf, grouper='julian_hr',)\n",
    "julian_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_station_quantiles` returns a `DataFrame` with\n",
    " * spatial coordinates `x` and `y`\n",
    " * columns related to clear sky solar radiation (columns with `_csky_` as a token)\n",
    " * measured solar radiation (columns without `_csky_` as a token)\n",
    " * some date / time related columns helpful for `groupby` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julian_summary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen(station=None, grouper='julian_hr', usaf_data=None):\n",
    "    '''Given a station name or dataframe do groupby on time bins\n",
    "    Parameters:\n",
    "        station:    Integer name of a USAF weather station \n",
    "                    (folder names holding years' CSVs)\n",
    "        groupby:    One of \"julian_hr\" \"hour\" \"month_hour\"\n",
    "        usaf_data:  Give CSVs' dataframe instead of station name\n",
    "    Returns:\n",
    "        curves:     Dictionary of hv.Curve objects showing \n",
    "                    25%, 50%, 75% percentiles\n",
    "    '''\n",
    "    summary_df = get_station_quantiles(station=station, \n",
    "                                       grouper=grouper, \n",
    "                                       usaf_data=usaf_data)\n",
    "    curves = {}\n",
    "    kw = dict(style=dict(s=2,alpha=0.5))\n",
    "    for col, label in zip(watt_hrs_m2_cols, labels):\n",
    "        dates = pd.DatetimeIndex(start=pd.Timestamp('2001-01-01'),\n",
    "                                 freq='H', \n",
    "                                 periods=summary_df.shape[0])\n",
    "        median_col = summary_df[col]\n",
    "        low_col = summary_df[col + '_low']\n",
    "        hi_col = summary_df[col + '_hi']\n",
    "        hi = hv.Curve((dates, hi_col), label=label + ' (upper quartile)')(**kw)\n",
    "        low = hv.Curve((dates, low_col),label=label + ' (lower quartile)')(**kw)\n",
    "        median = hv.Curve((dates, median_col), label=label)(**kw)\n",
    "        plot_id = tuple(col.replace('metstat_', '').replace('_wh_m_2', '').split('_'))\n",
    "        curves[plot_id] = low * median * hi\n",
    "        curves[plot_id].group = labels[col]\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `plot_gen` (function above) with an example USAF station to get a dictionary of `holoviews.Curve` objects that have been combined with the overloaded `holoviews` `*` operator for `Curves` or other `holoviews.element` objects.  The `*` operator is used to show 25%, 50%, and 75% time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_of_year = plot_gen(station=example_usaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dictionary with short keys for different plots of 25%, 50%, 75% of:\n",
    " * `(glo,)`: Measured Global Horizontal\n",
    " * `(dir,)`: Measured Direct Normal\n",
    " * `(dif,)`: Measured Diffuse Horizontal\n",
    " * `('csky', 'glo')`: Clear Sky Global Horizontal\n",
    " * `('csky', 'dir')`: Clear Sky Direct Normal\n",
    " * `('csky', 'dif')`: Clear Sky Diffuse Horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(hour_of_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=700 height=500]\n",
    "%%opts Layout [tabs=True]\n",
    "hour_of_year[('dir',)] + hour_of_year[('csky', 'dir')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=700 height=500 ]\n",
    "%%opts Layout [tabs=True]\n",
    "hour_of_year[('glo',)] + hour_of_year[('csky', 'glo')] + hour_of_year[('dif',)] + hour_of_year[('csky', 'dif',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells repeat the groupby operations for hour of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaf_data = read_one_station(example_usaf)\n",
    "#hour_of_day = plot_gen(grouper='hour', usaf_data=usaf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=700 height=500]\n",
    "%%opts Layout [tabs=True]\n",
    "#hour_of_day[('dir',)] + hour_of_day[('csky', 'dir')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When grouping by hour of day or month of year, the number of groups on the horizontal axis is small enough for box plots to show distributions legibly.  The next cell uses `holoviews.BoxWhisker` plots to show the direct normal radiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts BoxWhisker [width=600 height=600]\n",
    "%%opts Layout [tabs=True]\n",
    "(hv.BoxWhisker(usaf_data, kdims=['hour'], vdims=['metstat_dir_wh_m_2'],\n",
    "               group='Direct Normal - Hour of Day') +\n",
    " hv.BoxWhisker(usaf_data, kdims=['month'], vdims=['metstat_dir_wh_m_2'],\n",
    "               group='Direct Normal - Month of Year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
