{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install instructions:\n",
    "```\n",
    "conda install numpy pandas xarray dask colorcet datashader streamz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed the NYC taxi data from the notebook examples is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.colors import viridis\n",
    "\n",
    "from colorcet import fire\n",
    "from streamz import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def taxi_trips_stream(source='data/nyc_taxi.csv', frequency='T'):\n",
    "    \"\"\"Generate dataframes grouped by given frequency\"\"\"\n",
    "    def get_group(resampler, key):\n",
    "        try:\n",
    "            df = resampler.get_group(key)\n",
    "            df.reset_index(drop=True)\n",
    "        except KeyError:\n",
    "            df = pd.DataFrame()\n",
    "        return df\n",
    "\n",
    "    df = pd.read_csv(source,\n",
    "                     infer_datetime_format=True,\n",
    "                     parse_dates=['tpep_pickup_datetime', 'tpep_pickup_datetime'])\n",
    "    df = df.set_index('tpep_pickup_datetime', drop=True)\n",
    "    df = df.sort_index()\n",
    "    r = df.resample(frequency)\n",
    "    chunks = [get_group(r, g) for g in sorted(r.groups)]\n",
    "    indices = cycle(range(len(chunks)))\n",
    "    while True:\n",
    "        yield chunks[next(indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create streaming pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a stream of dataframes representing NYC taxi data, we create a pipeline with two streams. For each stream, the general steps are 1) aggregate each dataframe using Datashader reduction, 2) keep sliding window of aggregations, and 3) combine sliding window collection into image. The first stream creates a two-day sliding window aggregation, while the second stream creates a 1-week sliding window aggregation. The pipeline visualization below shows each step that makes up each stream.\n",
    "\n",
    "We use the primitives given in the `streamz` library to accomplish this. `aggregated_sliding_window_image_queue` creates each distinct pipeline, but this will likely be supplanted by a native `streamz.StreamingDataFrame` container when ready. Each stream will place its final aggregation into a double-ended queue, which is used to keep a history of previous aggregations. By default, we only keep the most recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fork_stream(source):\n",
    "    stream = Stream()\n",
    "    source.connect(stream)\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_df(df, x, y, plot_width=800, plot_height=600, agg=None):\n",
    "    cvs = ds.Canvas(plot_width=plot_width, plot_height=plot_height)\n",
    "    return cvs.points(df, x, y, agg)\n",
    "\n",
    "def aggregate_images(iterable, cmap):\n",
    "    merged = xr.concat(iterable, dim='cols')\n",
    "    total = merged.sum(dim='cols')\n",
    "    return tf.shade(total, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregated_sliding_window_image_queue(source, agg1, agg2, window=1, history=1):\n",
    "    q = deque(maxlen=history)\n",
    "    s = fork_stream(source).map(agg1).sliding_window(window)\n",
    "    s.map(agg2).sink(q.append)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for useful aggregations\n",
    "min_amount     = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.min('total_amount'))\n",
    "max_amount     = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.max('total_amount'))\n",
    "mean_amount    = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.mean('total_amount'))\n",
    "sum_amount     = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.sum('total_amount'))\n",
    "max_passengers = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.max('passenger_count'))\n",
    "sum_passengers = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.sum('passenger_count'))\n",
    "sum_pickups    = partial(aggregate_df, x='pickup_x', y='pickup_y', agg=ds.count())\n",
    "\n",
    "reduce_fire = partial(aggregate_images, cmap=fire)\n",
    "reduce_viridis = partial(aggregate_images, cmap=viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = Stream()\n",
    "q_days = aggregated_sliding_window_image_queue(source, window=2, history=10, agg1=max_amount, agg2=reduce_viridis)\n",
    "q_week = aggregated_sliding_window_image_queue(source, window=7, agg1=max_amount, agg2=reduce_viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push data through pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially push 7 days worth of dataframes through the pipeline since the sliding window requires a full window before emitting a window's worth of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_per_day = taxi_trips_stream(frequency='D')\n",
    "for i in range(7):\n",
    "    source.emit(next(trips_per_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_days[-1]  # most recent 2-day aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_week[-1]  # most recent 1-week aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.emit(next(trips_per_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_days[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_week[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
